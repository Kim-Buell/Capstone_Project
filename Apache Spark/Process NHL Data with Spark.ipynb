{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76da72-2bea-4178-bfec-d6f31c34a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import when, col, regexp_extract, expr, trim, lit, to_timestamp, collect_list, struct, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, TimestampType\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc8a0a-73d8-43e6-a0b2-0bb4052f678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_df(df):\n",
    "    \"\"\"Cleanse Spark NHL Raw DataFrame\"\"\" \n",
    "    # Change Montréal Canadiens name without accent\n",
    "    df = df.withColumn('Team Name', \n",
    "                       when(col('Team Name') == 'Montréal Canadiens', 'Montreal Canadiens')\n",
    "                       .otherwise(col('Team Name')))\n",
    "    \n",
    "    # Change \"St. Louis Blues\" to \"St Louis Blues\"\n",
    "    df = df.withColumn('Team Name', \n",
    "                       when(col('Team Name') == 'St. Louis Blues', 'St Louis Blues')\n",
    "                       .otherwise(col('Team Name')))\n",
    "    \n",
    "    # Drop Columns 'GF/GP' and 'GA/GP' since they are the same as 'GF' and 'GA'\n",
    "    df = df.drop('GF/GP').drop('GA/GP')\n",
    "    \n",
    "    # Replace \"--\" with 100 in PK and Net PK columns\n",
    "    df = df.withColumn('PK%', when(col('PK%') == '--', 100).otherwise(col('PK%')))\n",
    "    df = df.withColumn('Net PK%', when(col('Net PK%') == '--', 100).otherwise(col('Net PK%')))\n",
    "    \n",
    "    # Replace \"--\" with 0 in PP and Net PP columns\n",
    "    df = df.withColumn('PP%', when(col('PP%') == '--', 0).otherwise(col('PP%')))\n",
    "    df = df.withColumn('Net PP%', when(col('Net PP%') == '--', 0).otherwise(col('Net PP%')))\n",
    "    \n",
    "    # Replace \"N/A\" with NaNs\n",
    "    df = df.replace('N/A', None)\n",
    "    \n",
    "    # Convert selected columns to numeric type\n",
    "    numeric_columns = ['GP', 'W', 'L', 'P', 'P%', 'RW', 'ROW', 'SO_win', 'GF', 'GA', 'PP%', 'PK%',\n",
    "                       'Net PP%', 'Net PK%', 'Shots/GP', 'SA/GP', 'FOW%']\n",
    "    for col_name in numeric_columns:\n",
    "        df = df.withColumn(col_name, df[col_name].cast(IntegerType()))\n",
    "    \n",
    "    # Calculate \"Save %\" and \"Shooting &\"\n",
    "    df = df.withColumn('Save %', (col('SA/GP') - col('GA')) / col('SA/GP'))\n",
    "    df = df.withColumn('Shooting %', col('GF') / col('Shots/GP'))\n",
    "    \n",
    "    # Calculate Corsi %\n",
    "    df = df.withColumn('Corsi%', (col('Shots/GP') / (col('Shots/GP') + col('SA/GP'))) * 100)\n",
    "    \n",
    "    # Extract the abbreviation for the opposing team\n",
    "    opponent_abbr = regexp_extract(col('Game Date'), r'@ (\\w{3})|vs (\\w{3})', 1)\n",
    "    \n",
    "    # Assign the extracted values to the 'Against Team' column\n",
    "    df = df.withColumn('Against Team', \n",
    "                       when(opponent_abbr != '', opponent_abbr)\n",
    "                       .otherwise(regexp_extract(col('Game Date'), r'@ (\\w{3})|vs (\\w{3})', 2)))\n",
    "    \n",
    "    # JDBC connection properties for PostgreSQL\n",
    "    db_properties = {\n",
    "        \"user\": \"User_1\",\n",
    "        \"password\": \"postgres\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    # JDBC URL for PostgreSQL\n",
    "    jdbc_url = \"jdbc:postgresql://192.168.1.246:5432/nhl_master_data\"\n",
    "\n",
    "    # Create teamname_abbreviations_df\n",
    "    teamname_abbreviations_df = spark.read.jdbc(url=jdbc_url, table=\"teamname_abbreviations\", properties=db_properties)\n",
    "    \n",
    "    # Joining the DataFrames to map abbreviations to full team names\n",
    "    df = df.join(\n",
    "        teamname_abbreviations_df,\n",
    "        col(\"Against Team\") == col(\"abbreviation\"),\n",
    "        \"left\"\n",
    "    ).drop(teamname_abbreviations_df[\"abbreviation\"])\n",
    "    \n",
    "    # Replacing the \"Against Team\" column with the full team names\n",
    "    df = df.withColumn(\"Against Team\", col(\"Full Name\"))\n",
    "    \n",
    "    # Selecting the necessary columns and showing the DataFrame\n",
    "    df = df.select(\"Team Name\", \"Game Date\", \"GP\", \"W\", \"L\", \"T\", \"OT\", \"P\", \"P%\", \"RW\", \"ROW\", \"SO_win\", \"GF\", \"GA\", \"PP%\", \"PK%\", \"Net PP%\", \"Net PK%\", \"Shots/GP\", \"SA/GP\", \"FOW%\", \"Season\", \"Type\", \"Save %\", \"Shooting %\", \"Corsi%\", \"Against Team\")\n",
    "    \n",
    "    # Define a function to determine home and away teams\n",
    "    def determine_home_away(Game_Date, Team_Name, Against_Team):\n",
    "        if \"@\" in Game_Date:\n",
    "            return Against_Team, Team_Name\n",
    "        elif \"vs\" in Game_Date:\n",
    "            return Team_Name, Against_Team\n",
    "    \n",
    "    # Apply the function to create new columns\n",
    "    df = df.withColumn('Home Team', when(col('Game Date').contains('@'), col('Against Team')).otherwise(col('Team Name')))\n",
    "    df = df.withColumn('Away Team', when(col('Game Date').contains('@'), col('Team Name')).otherwise(col('Against Team')))\n",
    "    \n",
    "    # Apply the regexp_extract function to the 'Game Date' column\n",
    "    df = df.withColumn('Game Date', regexp_extract(col('Game Date'), r'\\d{4}/\\d{2}/\\d{2}', 0))\n",
    "\n",
    "    # Convert 'Game Date' to TimestampType\n",
    "    df = df.withColumn(\"Game Date\", to_timestamp(\"Game Date\", \"yyyy/MM/dd\"))\n",
    "    \n",
    "    # Create a new column 'Home Team W' initialized with 0\n",
    "    df = df.withColumn('Home Team W', lit(0))\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    df = df.withColumn('Home Team W', when((col('Team Name') == col('Home Team')) & (col('W') == 1), 1)\n",
    "                       .when((col('Team Name') == col('Away Team')) & (col('W') == 0), 1)\n",
    "                       .otherwise(0))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941d864-7f4a-4b51-9698-afd6929893c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_n_days_rolling_stats(group_df, n_values):\n",
    "    columns_to_avg = ['P%', 'Corsi%']\n",
    "    \n",
    "    for n in n_values:\n",
    "        for column in columns_to_avg:\n",
    "            # Define the window frame\n",
    "            window = Window.partitionBy('Team Name').orderBy(col('Game Date')).rowsBetween(-n, -1)\n",
    "            \n",
    "            # Apply the window function to calculate rolling average\n",
    "            rolling_avg_column = avg(column).over(window)\n",
    "            \n",
    "            # Apply the window function to the DataFrame\n",
    "            group_df = group_df.withColumn(f'{column}_NDays_Rolling_Avg_{n}', rolling_avg_column)\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "def cast_numeric_columns(row, schema):\n",
    "    \"\"\"Cast numeric columns to appropriate types\"\"\"\n",
    "    updated_values = []\n",
    "    for field in schema.fields:\n",
    "        value = row[field.name]\n",
    "        if isinstance(value, int) and isinstance(field.dataType, DoubleType):\n",
    "            value = float(value)\n",
    "        updated_values.append(value)\n",
    "    return Row(*updated_values)\n",
    "\n",
    "def update_teams_tables(grouped, schema, db_string):\n",
    "    \"\"\"Update Teams' Tables in PostgreSQL with New Data and Rolling Averages/Sums\"\"\"\n",
    "\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Update Teams Tables\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    for row in grouped.collect():  # collect() is used to convert GroupedData to a DataFrame\n",
    "        team = row['Team Name']\n",
    "        group_df = row['group_df']\n",
    "        \n",
    "        # Check if group_df is not empty and is a list of rows\n",
    "        if group_df:\n",
    "            # Cast numeric columns to appropriate types\n",
    "            group_df = [cast_numeric_columns(row, schema) for row in group_df]\n",
    "            \n",
    "            # Explode the nested DataFrame\n",
    "            group_df = spark.createDataFrame(group_df, schema)\n",
    "\n",
    "            # Replace zeros with float(0) in columns of DoubleType\n",
    "            for field in schema.fields:\n",
    "                if field.dataType == DoubleType():\n",
    "                    group_df = group_df.withColumn(field.name, \\\n",
    "                        when(col(field.name) == 0, float(0)).otherwise(col(field.name)))\n",
    "\n",
    "            # Add the following columns to group_df DataFrame\n",
    "            group_df = group_df.withColumn(\"P%_NDays_Rolling_Avg_35\", lit(float(0)).cast(DoubleType()))\n",
    "            group_df = group_df.withColumn(\"Corsi%_NDays_Rolling_Avg_35\", lit(float(0)).cast(DoubleType()))\n",
    "            group_df = group_df.withColumn(\"P%_NDays_Rolling_Avg_40\", lit(float(0)).cast(DoubleType()))\n",
    "            group_df = group_df.withColumn(\"Corsi%_NDays_Rolling_Avg_40\", lit(float(0)).cast(DoubleType()))\n",
    "            group_df = group_df.withColumn(\"P%_NDays_Rolling_Avg_50\", lit(float(0)).cast(DoubleType()))\n",
    "            group_df = group_df.withColumn(\"Corsi%_NDays_Rolling_Avg_50\", lit(float(0)).cast(DoubleType()))\n",
    "\n",
    "            # Read existing data from the team's table\n",
    "            existing_data = spark.read.format(\"jdbc\").options(\n",
    "                url=db_string,\n",
    "                dbtable=f\"team_{team.replace(' ', '_').lower()}\",\n",
    "                user=\"User_1\",\n",
    "                password=\"postgres\").load()\n",
    "            \n",
    "            # Union existing data with new data\n",
    "            merged_data = existing_data.union(group_df)\n",
    "\n",
    "            # Order the DataFrame by the timestamp column in ascending order\n",
    "            merged_data = merged_data.orderBy(col(\"Game Date\").asc())\n",
    "\n",
    "            # Calculate Merged Data\n",
    "            merged_data = calculate_n_days_rolling_stats(merged_data, [35,40,50])\n",
    "            \n",
    "            # Filter merged_data to include only rows where 'Game Date' is in group_df\n",
    "            group_df_dates = [row['Game Date'] for row in group_df.select('Game Date').distinct().collect()]\n",
    "            merged_data = merged_data.filter(col('Game Date').isin(group_df_dates))\n",
    "\n",
    "            # Write the DataFrame to the database, replacing the table\n",
    "            db_properties = {\n",
    "                \"user\": \"User_1\", \n",
    "                \"password\": \"postgres\",\n",
    "                \"driver\": \"org.postgresql.Driver\"\n",
    "            }\n",
    "            \n",
    "            db_url = \"jdbc:postgresql://192.168.1.246:5432/nhl_team_data\"\n",
    "            \n",
    "            merged_data.write.jdbc(url=db_url, table=f\"team_{team.replace(' ', '_').lower()}\", mode=\"append\", properties=db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7fd9c-2315-41c4-9263-bdca9c1ef7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_data(df):\n",
    "    from pyspark.sql.functions import col as spark_col\n",
    "    \n",
    "    # Merge home and away teams\n",
    "    home_teams = df.filter(df['Team Name'] == df['Home Team'])\n",
    "    away_teams = df.filter(df['Team Name'] == df['Away Team'])\n",
    "    \n",
    "    # Select the \"Team Name\" column and get distinct values\n",
    "    unique_team_names = df.select(\"Team Name\").distinct()\n",
    "    \n",
    "    # Collect the distinct team table names as a list with the desired format\n",
    "    table_names = ['team_' + row[\"Team Name\"].replace(\" \", \"_\").lower().replace(\".\", \"\") for row in unique_team_names.collect()]\n",
    "    \n",
    "    # Define the database connection properties\n",
    "    db_string = \"jdbc:postgresql://192.168.1.246:5432/nhl_team_data\"\n",
    "    \n",
    "    # Read each table and union them\n",
    "    combined_team_df = None\n",
    "    for table_name in table_names:\n",
    "        team_df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", db_string) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", \"User_1\") \\\n",
    "            .option(\"password\", \"postgres\") \\\n",
    "            .load()\n",
    "        if combined_team_df is None:\n",
    "            combined_team_df = team_df\n",
    "        else:\n",
    "            combined_team_df = combined_team_df.union(team_df)\n",
    "            \n",
    "    # Select columns from combined_team_df that contain \"Rolling\" in the column header\n",
    "    selected_columns = ['Team Name', 'Game Date'] + [col for col in combined_team_df.columns if 'Rolling' in col]\n",
    "    combined_team_df = combined_team_df.select(*selected_columns)\n",
    "    \n",
    "    # Merge Home and Away Teams with Combined Rolling Dataframe Based on 'Team Name' and 'Game Date'\n",
    "    home_merged_df = home_teams.join(combined_team_df, on=['Team Name', 'Game Date'], how='left')\n",
    "    away_merged_df = away_teams.join(combined_team_df, on=['Team Name', 'Game Date'], how='left')\n",
    "    \n",
    "    # Remove Unnecessary Columns\n",
    "    home_selected_columns = ['Game Date', 'Home Team', 'Away Team', 'Home Team W'] + [col for col in home_merged_df.columns if 'Rolling' in col]\n",
    "    home_merged_df = home_merged_df.select(*home_selected_columns)\n",
    "    \n",
    "    away_selected_columns = ['Game Date', 'Home Team', 'Away Team', 'Home Team W'] + [col for col in away_merged_df.columns if 'Rolling' in col]\n",
    "    away_merged_df = away_merged_df.select(*away_selected_columns)\n",
    "    \n",
    "    # For Home Teams iterate over column names and add \"Home_\" to columns containing \"Rolling\" in the header\n",
    "    for column in home_merged_df.columns:\n",
    "        if 'Rolling' in column:\n",
    "            home_merged_df = home_merged_df.withColumnRenamed(column, 'Home_' + column)\n",
    "    \n",
    "    # For Away Teams iterate over column names and add \"Away_\" to columns containing \"Rolling\" in the header\n",
    "    for column in away_merged_df.columns:\n",
    "        if 'Rolling' in column:\n",
    "            away_merged_df = away_merged_df.withColumnRenamed(column, 'Away_' + column)\n",
    "    \n",
    "    # Drop the \"Team Name\" column from home_merged_df and away_merged_df\n",
    "    home_merged_df = home_merged_df.drop(\"Team Name\")\n",
    "    away_merged_df = away_merged_df.drop(\"Team Name\")\n",
    "    \n",
    "    # Join the Home and Away Rolling DataFrames\n",
    "    join_columns = [\"Game Date\", \"Home Team\", \"Away Team\", \"Home Team W\"]\n",
    "    rolling_df = home_merged_df.join(away_merged_df, on=join_columns, how=\"inner\")\n",
    "    \n",
    "    # Define the list of rolling column names for Home and Away\n",
    "    home_rolling_columns = ['Home_P%_NDays_Rolling_Avg_35', 'Home_Corsi%_NDays_Rolling_Avg_35',\n",
    "                            'Home_P%_NDays_Rolling_Avg_40', 'Home_Corsi%_NDays_Rolling_Avg_40',\n",
    "                            'Home_P%_NDays_Rolling_Avg_50', 'Home_Corsi%_NDays_Rolling_Avg_50']\n",
    "    \n",
    "    away_rolling_columns = ['Away_P%_NDays_Rolling_Avg_35', 'Away_Corsi%_NDays_Rolling_Avg_35',\n",
    "                            'Away_P%_NDays_Rolling_Avg_40', 'Away_Corsi%_NDays_Rolling_Avg_40',\n",
    "                            'Away_P%_NDays_Rolling_Avg_50', 'Away_Corsi%_NDays_Rolling_Avg_50']\n",
    "    \n",
    "    # Create difference columns for each pair of Home and Away rolling columns\n",
    "    for home_col, away_col in zip(home_rolling_columns, away_rolling_columns):\n",
    "        # Generate the name for the difference column\n",
    "        difference_column_name = home_col.replace(\"Home_\", \"Difference_\")\n",
    "        \n",
    "        # Subtract the Away rolling column from the Home rolling column and create the difference column\n",
    "        rolling_df = rolling_df.withColumn(difference_column_name, spark_col(home_col) - spark_col(away_col))\n",
    "    \n",
    "    # Select Final Columns \n",
    "    selected_columns = ['Game Date', 'Home Team', 'Away Team', 'Home Team W'] + [col for col in rolling_df.columns if 'Difference' in col]\n",
    "    rolling_df = rolling_df.select(selected_columns)\n",
    "    \n",
    "    # Convert 'Home Team W' column to 'bigint'\n",
    "    rolling_df = rolling_df.withColumn('Home Team W', spark_col('Home Team W').cast('bigint'))\n",
    "\n",
    "    rolling_df.show()\n",
    "\n",
    "    # Save the Model Data\n",
    "    db_properties = {\n",
    "        \"user\": \"User_1\", \n",
    "        \"password\": \"postgres\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    \n",
    "    db_url = \"jdbc:postgresql://192.168.1.246:5432/nhl_master_data\"\n",
    "    \n",
    "    rolling_df.write.jdbc(url=db_url, table=\"update_model_data\", mode=\"append\", properties=db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da2cd0-9f63-4b93-a234-966de72e34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Streamed Kafka Data\n",
    "def process_kafka_data(batch_df, batch_id):\n",
    "    if not batch_df.isEmpty():\n",
    "        values = [row.value for row in batch_df.collect()]\n",
    "        for value in values:\n",
    "            # Parse JSON string into dictionary\n",
    "            data_dict = json.loads(value)\n",
    "            \n",
    "            # Create DataFrame from dictionary\n",
    "            df = pd.DataFrame.from_dict(data_dict)\n",
    "            df = spark.createDataFrame(df)\n",
    "            df.show()\n",
    "\n",
    "            # Define schema for group_df\n",
    "            schema = StructType([\n",
    "                StructField(\"Team Name\", StringType(), nullable=True),\n",
    "                StructField(\"Game Date\", TimestampType(), nullable=True),\n",
    "                StructField(\"GP\", LongType(), nullable=True),\n",
    "                StructField(\"W\", LongType(), nullable=True),\n",
    "                StructField(\"L\", LongType(), nullable=True),\n",
    "                StructField(\"T\", StringType(), nullable=True),\n",
    "                StructField(\"OT\", StringType(), nullable=True),\n",
    "                StructField(\"P\", LongType(), nullable=True),\n",
    "                StructField(\"P%\", DoubleType(), nullable=True),\n",
    "                StructField(\"RW\", LongType(), nullable=True),\n",
    "                StructField(\"ROW\", LongType(), nullable=True),\n",
    "                StructField(\"SO_win\", LongType(), nullable=True),\n",
    "                StructField(\"GF\", LongType(), nullable=True),\n",
    "                StructField(\"GA\", LongType(), nullable=True),\n",
    "                StructField(\"PP%\", DoubleType(), nullable=True),\n",
    "                StructField(\"PK%\", DoubleType(), nullable=True),\n",
    "                StructField(\"Net PP%\", DoubleType(), nullable=True),\n",
    "                StructField(\"Net PK%\", DoubleType(), nullable=True),\n",
    "                StructField(\"Shots/GP\", DoubleType(), nullable=True),\n",
    "                StructField(\"SA/GP\", DoubleType(), nullable=True),\n",
    "                StructField(\"FOW%\", DoubleType(), nullable=True),\n",
    "                StructField(\"Season\", StringType(), nullable=True),\n",
    "                StructField(\"Type\", StringType(), nullable=True),\n",
    "                StructField(\"Save %\", DoubleType(), nullable=True),\n",
    "                StructField(\"Shooting %\", DoubleType(), nullable=True),\n",
    "                StructField(\"Corsi%\", DoubleType(), nullable=True),\n",
    "                StructField(\"Against Team\", StringType(), nullable=True),\n",
    "                StructField(\"Home Team\", StringType(), nullable=True),\n",
    "                StructField(\"Away Team\", StringType(), nullable=True),\n",
    "                StructField(\"Home Team W\", LongType(), nullable=True)\n",
    "            ])\n",
    "            \n",
    "            # Cleanse Data From Kafka\n",
    "            df = cleanse_df(df)\n",
    "            \n",
    "            # Group data by 'Team Name' and aggregate using collect_list\n",
    "            grouped = df.groupBy('Team Name').agg(collect_list(struct(df.columns)).alias('group_df'))\n",
    "            \n",
    "            # Define the database connection properties\n",
    "            db_string = \"jdbc:postgresql://192.168.1.246:5432/nhl_team_data\"\n",
    "            \n",
    "            # Update teams' tables\n",
    "            update_teams_tables(grouped, schema, db_string)\n",
    "            \n",
    "            # Create model data\n",
    "            create_model_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03c105-5e93-4459-b572-5087b1ea1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka_NHL_tream\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Kafka configurations\n",
    "kafka_bootstrap_servers = \"192.168.1.246:9092\"\n",
    "kafka_topic = \"spark_test\"\n",
    "\n",
    "# Read from Kafka topic as a streaming DataFrame\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from binary to string\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Process Kafka Data\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(process_kafka_data) \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination of the query\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86adf825-1dc9-48f9-8546-6d7c96b3e2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
